
# ğŸ§ª Lab 02 â€“ Chunking Strategy Benchmark

## ğŸ¯ Goal

Determine optimal chunk size and overlap for retrieval quality using measurable metrics.

---

## ğŸ”¬ What We Are Testing

Chunk parameters:

* chunk_size âˆˆ {400, 800, 1200}
* overlap âˆˆ {50, 150, 250}

We isolate variables:

* Same PDF
* Same embedding model
* Same eval queries
* Only chunking changes

---

## ğŸ“Š Evaluation Metrics

### 1ï¸âƒ£ hit@k

Did the correct page appear in the top k (mostly 5) results?

---

### 2ï¸âƒ£ MRR (Mean Reciprocal Rank)

Measures ranking quality (from 0 to 1).

MRR rewards higher placement.

---

## ğŸ§  Why Lab 02 Matters

Without benchmarking:

* Chunk size is guessing.
* Overlap is guessing.
* Retrieval quality is unknown.

With benchmarking:

* We choose defaults using evidence.
* We optimize retrieval before generation.
* We learn how structure affects search.

---

## ğŸ” Synthetic Evaluation Design

Evaluation queries are auto-generated:

1. Pick random page.
2. Pick medium-length sentence.
3. Use it as query.
4. Gold label = that page.

This allows consistent comparison between chunk configs.

---

## ğŸ Expected Output

For each config:

```
chunk_size  overlap   hit@5   mrr@5
400        150      0.667  0.462  â† Best
400        50       0.500  0.458
800        150      0.500  0.381
800        250      0.444  0.361
1200       150      0.444  0.266
1200       250      0.389  0.247
```

---

#### Observations from run:
![lab-2-run](data/screenshots/image-2.png)